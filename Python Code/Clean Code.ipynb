{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad4e6b4-7435-43b9-8964-57f6e98d6065",
   "metadata": {},
   "source": [
    "# Film list scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09a310-9775-41fd-9656-7a48a12152d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the webpage to scrape\n",
    "film_url = \"https://www.yorck.de/en/films\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "film_response = requests.get(film_url)\n",
    "film_soup = BeautifulSoup(film_response.content, \"html.parser\")\n",
    "\n",
    "film_titles = film_soup.select(\"h3.MuiTypography-h3.whatson-film-title\")\n",
    "\n",
    "clean_titles = []\n",
    "\n",
    "# Extract and store the clean film titles\n",
    "for title in film_titles:\n",
    "    clean_title = title.get_text(strip=True)\n",
    "    clean_titles.append(clean_title)\n",
    "\n",
    "# print(clean_titles) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0efc2-c4d9-4ed7-b711-19f3f37e2512",
   "metadata": {},
   "source": [
    "# IMDB API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35583e9e-8f2a-4233-86a6-9831531db969",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets_file = open(\"imdb_api_key.txt\")\n",
    "api_key = secrets_file.read()\n",
    "api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68996eb0-2d23-4c98-979b-a913b04ae6cb",
   "metadata": {},
   "source": [
    "# Film table building using the IMDB API\n",
    "There is a more usesful function lower down called \"Collecting film data\" which will work for target and current films.\n",
    "Will leave this one for longevity and because it still works fine, just requires addiitonal steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e22cb-2895-421d-ad99-b8af30c583b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import time\n",
    "# import pandas as pd\n",
    "\n",
    "# # RapidAPI details\n",
    "# api_host = \"imdb8.p.rapidapi.com\"\n",
    "\n",
    "# # Base URL for search\n",
    "# search_url = \"https://imdb8.p.rapidapi.com/v2/search\"\n",
    "# # Base URL for getting ratings\n",
    "# ratings_url = \"https://imdb8.p.rapidapi.com/title/v2/get-ratings\"\n",
    "\n",
    "# # Headers for the API requests\n",
    "# headers = {\n",
    "#     \"X-RapidAPI-Key\": api_key,\n",
    "#     \"X-RapidAPI-Host\": api_host\n",
    "# }\n",
    "\n",
    "# # List to store the results\n",
    "# results = []\n",
    "\n",
    "# # Loop through the list of titles\n",
    "# for title in clean_titles:\n",
    "#     # Query string for the search API\n",
    "#     querystring = {\"searchTerm\": title, \"type\": \"MOVIE\", \"first\": \"1\", \"country\": \"US\", \"language\": \"en-US\"}\n",
    "\n",
    "#     # Make the API call to search for the title\n",
    "#     response = requests.get(search_url, headers=headers, params=querystring)\n",
    "    \n",
    "#     # Adding delay between API calls\n",
    "#     time.sleep(1)\n",
    "    \n",
    "#     if response.ok:\n",
    "#         try:\n",
    "#             data = response.json()\n",
    "#             edges = data[\"data\"][\"mainSearch\"][\"edges\"]\n",
    "#             film_id = None\n",
    "#             image_url = None\n",
    "#             for edge in edges:\n",
    "#                 node = edge.get(\"node\", {})\n",
    "#                 entity = node.get(\"entity\", {})\n",
    "#                 if entity.get(\"__typename\") == \"Title\":\n",
    "#                     film_id = entity.get(\"id\", \"\")\n",
    "#                     primary_image = entity.get(\"primaryImage\", {})\n",
    "#                     image_url = primary_image.get(\"url\", \"\")\n",
    "#                     break\n",
    "#         except (KeyError, json.JSONDecodeError) as e:\n",
    "#             print(f\"Error processing search response for {title}: {e}\")\n",
    "#             film_id = None\n",
    "#             image_url = None\n",
    "#     else:\n",
    "#         print(f\"Error: {response.status_code} - {response.reason}\")\n",
    "#         film_id = None\n",
    "#         image_url = None\n",
    "    \n",
    "#     if film_id is not None:\n",
    "#         querystring2 = {\"tconst\": film_id, \"country\": \"US\", \"language\": \"en-US\"}\n",
    "\n",
    "#         response2 = requests.get(ratings_url, headers=headers, params=querystring2)\n",
    "        \n",
    "#         # Adding delay between API calls\n",
    "#         time.sleep(5)\n",
    "        \n",
    "#         if response2.ok:\n",
    "#             try:\n",
    "#                 data2 = response2.json()\n",
    "#                 aggregate_rating = data2[\"data\"][\"title\"][\"ratingsSummary\"][\"aggregateRating\"]\n",
    "#             except (KeyError, json.JSONDecodeError) as e:\n",
    "#                 print(f\"Error processing ratings response for {film_id}: {e}\")\n",
    "#                 aggregate_rating = None\n",
    "#         else:\n",
    "#             print(f\"Error: {response2.status_code} - {response2.reason}\")\n",
    "#             aggregate_rating = None\n",
    "        \n",
    "#         # Store the result in the results list\n",
    "#         results.append({\n",
    "#             \"Title\": title,\n",
    "#             \"Film ID\": film_id,\n",
    "#             \"Image URL\": image_url,\n",
    "#             \"Aggregate Rating\": aggregate_rating\n",
    "#         })\n",
    "#     else:\n",
    "#         print(f\"Film ID not found for {title}\")\n",
    "\n",
    "# # Create a DataFrame from the results\n",
    "# rated_films = pd.DataFrame(results)\n",
    "\n",
    "# # filter the responses\n",
    "# top_rated_films = rated_films[rated_films['Aggregate Rating'] > 6.5]\n",
    "\n",
    "# top_rated_films"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63ea4c-c0ea-46b1-9bb3-add94357a01e",
   "metadata": {},
   "source": [
    " # Creating the museum dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72897d-4e6e-4d59-896a-4470ea8c4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "history_museum = 'https://www.museumsportal-berlin.de/en/program?exhibition_type=special&category=geschichte-kulturgeschichte'\n",
    "art_museum = 'https://www.museumsportal-berlin.de/en/program?exhibition_type=special&category=architektur-kunstgewerbe-design&category=architektur-kunstgewerbe-design&category=architektur-kunstgewerbe-design&category=architektur-kunstgewerbe-design&category=architektur-kunstgewerbe-design&category=bildende-kunst'\n",
    "design_museum = 'https://www.museumsportal-berlin.de/en/program?exhibition_type=special&category=architektur-kunstgewerbe-design'\n",
    "\n",
    "def scrape_museum_data(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll and load more data\n",
    "    for _ in range(10):  # Adjust the range for the number of scrolls\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        time.sleep(5)  # Adjust the sleep time to allow data to load\n",
    "\n",
    "    # Get the page source after scrolling\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Initialize lists to store the extracted data\n",
    "    types = []\n",
    "    locations = []\n",
    "    h2_titles = []\n",
    "    h3_titles = []\n",
    "    times = []\n",
    "\n",
    "    # Loop through each card and extract the necessary information\n",
    "    for card in soup.find_all('mp-card'):\n",
    "        # Extract the type\n",
    "        card_type = card.find('p', class_='mp-card-type').get_text(strip=True)\n",
    "        types.append(card_type)\n",
    "        \n",
    "        # Extract the location\n",
    "        location = card.find('p', class_='mp-card-location').get_text(strip=True)\n",
    "        locations.append(location)\n",
    "        \n",
    "        # Extract the h2 title\n",
    "        h2_title = card.find('h2').get_text(strip=True)\n",
    "        h2_titles.append(h2_title)\n",
    "        \n",
    "        # Extract the h3 title\n",
    "        h3_title = card.find('h3')\n",
    "        h3_titles.append(h3_title.get_text(strip=True) if h3_title else \"\")\n",
    "        \n",
    "        # Extract the time\n",
    "        time_element = card.find('time')\n",
    "        times.append(time_element.get_text(strip=True) if time_element else \"Closed this day\")\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({\n",
    "        'Type': types,\n",
    "        'Museum': locations,\n",
    "        'Name': h2_titles,\n",
    "        'Description': h3_titles,\n",
    "        'time': times\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    history_museum,  # Replace with actual URL\n",
    "    art_museum,  # Replace with actual URL\n",
    "    design_museum   # Replace with actual URL\n",
    "]\n",
    "\n",
    "# Scrape each URL and store the resulting DataFrames in a list\n",
    "dataframes = [scrape_museum_data(url) for url in [history_museum, art_museum, design_museum]]\n",
    "\n",
    "hist_exhib = dataframes[0]\n",
    "art_exhib = dataframes[1]\n",
    "design_exhib = dataframes[2]\n",
    "\n",
    "value1 = 'design'\n",
    "design_exhib['tag'] = value1\n",
    "# design_exhib\n",
    "value2 = 'art'\n",
    "art_exhib['tag'] = value2\n",
    "# art_exhib\n",
    "value3 = 'history'\n",
    "hist_exhib['tag'] = value3\n",
    "# hist_exhib\n",
    "\n",
    "combined_museums = pd.concat([design_exhib, art_exhib, hist_exhib], ignore_index=True)\n",
    "# combined_museums\n",
    "\n",
    "duplicate_rows = combined_museums[combined_museums.duplicated(subset=['h2'])]\n",
    "# duplicate_rows\n",
    "tagged_museum_recs = duplicate_rows.drop_duplicates(subset=['h2'])\n",
    "clean_museum_recs = tagged_museum_recs[tagged_museum_recs['time'] != 'Closed this day']\n",
    "# clean_museum_recs\n",
    "\n",
    "# This really has to be updated to include only the months you need. There must be better code for this and to include only one month rather than excluding all\n",
    "search_strings = ['July', 'August', 'September','October','November','December', ['children', 'kinder']]\n",
    "\n",
    "# Create a regex pattern to match any of the search strings\n",
    "pattern = '|'.join(search_strings)\n",
    "\n",
    "# Filter rows where 'description' contains any of the search strings\n",
    "open_museum_recs = clean_museum_recs[~clean_museum_recs['time'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "# open_museum_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17333d74-4491-45be-b3d0-56addec4b653",
   "metadata": {},
   "source": [
    "# Save museum recs to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21904d-abe3-4766-8fe0-68d2c0790f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_museum_recs.to_json('museum_recs.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d06841-973d-4c9b-852b-6dcf155227b1",
   "metadata": {},
   "source": [
    "# Create events df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914fb476-e774-4d0e-a5fd-88eb9a1a16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_url = \"https://www.berlin.de/events/\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "events_response = requests.get(events_url)\n",
    "events_soup = BeautifulSoup(events_response.content, \"html.parser\")\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "titles = []\n",
    "dates = []\n",
    "descriptions = []\n",
    "links = []\n",
    "images = []\n",
    "\n",
    "# Loop through each article and extract the necessary information\n",
    "for article in events_soup.find_all('article', class_='modul-teaser'):\n",
    "    # Extract the title\n",
    "    title_tag = article.find('h3', class_='title')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else 'No title'\n",
    "    titles.append(title)\n",
    "    \n",
    "    # Extract the date\n",
    "    date_tag = article.find('p', class_='teaser__meta')\n",
    "    date = date_tag.get_text(strip=True) if date_tag else 'No date'\n",
    "    dates.append(date)\n",
    "    \n",
    "    # Extract the description\n",
    "    description_tag = article.find('p', class_='text')\n",
    "    description = description_tag.get_text(strip=True).replace(' mehr', '') if description_tag else 'No description'\n",
    "    descriptions.append(description)\n",
    "    \n",
    "    # Extract the link\n",
    "    link_tag = title_tag.find('a') if title_tag else None\n",
    "    link = link_tag['href'] if link_tag else 'No link'\n",
    "    links.append(link)\n",
    "    \n",
    "    # Extract the image URL\n",
    "    image_tag = article.find('div', class_='image__image').find('img')\n",
    "    image = image_tag['src'] if image_tag else 'No image'\n",
    "    images.append(image)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "events_df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date': dates,\n",
    "    'Description': descriptions,\n",
    "    'Link': links,\n",
    "    'Image': images\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "# events_df\n",
    "\n",
    "current_events_df = events_df[events_df['Date'] != 'No date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06985e45-b671-433a-82f8-cc1f6726593e",
   "metadata": {},
   "source": [
    "# Creating the target events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae222a-5f82-44f5-b0e0-6bacc039bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_events_url = \"https://www.berlin.de/events/top-10/\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "tg_events_response = requests.get(target_events_url)\n",
    "target_events_soup = BeautifulSoup(tg_events_response.content, \"html.parser\")\n",
    "\n",
    "# Initialize lists to store the extracted data\n",
    "titles = []\n",
    "dates = []\n",
    "descriptions = []\n",
    "links = []\n",
    "images = []\n",
    "\n",
    "# Loop through each article and extract the necessary information\n",
    "for article in target_events_soup.find_all('article', class_='modul-teaser'):\n",
    "    # Extract the title\n",
    "    title_tag = article.find('h3', class_='title')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else 'No title'\n",
    "    titles.append(title)\n",
    "    \n",
    "    # Extract the date\n",
    "    date_tag = article.find('p', class_='teaser__meta')\n",
    "    date = date_tag.get_text(strip=True) if date_tag else 'No date'\n",
    "    dates.append(date)\n",
    "    \n",
    "    # Extract the description\n",
    "    description_tag = article.find('p', class_='text')\n",
    "    description = description_tag.get_text(strip=True).replace(' mehr', '') if description_tag else 'No description'\n",
    "    descriptions.append(description)\n",
    "    \n",
    "    # Extract the link\n",
    "    link_tag = title_tag.find('a') if title_tag else None\n",
    "    link = link_tag['href'] if link_tag else 'No link'\n",
    "    links.append(link)\n",
    "    \n",
    "    # Extract the image URL\n",
    "    image_tag = article.find('div', class_='image__image').find('img')\n",
    "    image = image_tag['src'] if image_tag else 'No image'\n",
    "    images.append(image)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "target_events_df = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Date': dates,\n",
    "    'Description': descriptions,\n",
    "    'Link': links,\n",
    "    'Image': images\n",
    "})\n",
    "\n",
    "target_events_df = target_events_df[target_events_df['Date'] != 'No date']\n",
    "# target_events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bb94c-4b53-4deb-8d01-43f546189474",
   "metadata": {},
   "source": [
    "# Building similarity score for current events vs the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1c096-b952-4f17-a788-c5f4d1ed2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Function to preprocess text: tokenize, lemmatize, and remove stop words\n",
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the descriptions\n",
    "current_events_df['processed_description'] = current_events_df['Description'].apply(preprocess)\n",
    "target_events_df['processed_description'] = target_events_df['Description'].apply(preprocess)\n",
    "\n",
    "# Combine descriptions with target descriptions\n",
    "all_descriptions = current_events_df['processed_description'].tolist() + target_events_df['processed_description'].tolist()\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform all descriptions\n",
    "tfidf_matrix = vectorizer.fit_transform(all_descriptions)\n",
    "\n",
    "# Separate the tfidf matrix into event descriptions and target descriptions\n",
    "event_tfidf_matrix = tfidf_matrix[:len(current_events_df)]\n",
    "target_tfidf_matrix = tfidf_matrix[len(current_events_df):]\n",
    "\n",
    "# Compute cosine similarity between event descriptions and target descriptions\n",
    "cosine_sim = cosine_similarity(event_tfidf_matrix, target_tfidf_matrix)\n",
    "\n",
    "# Function to get max similarity score for each event description\n",
    "def get_max_similarity_score(cosine_sim_row):\n",
    "    return max(cosine_sim_row)\n",
    "\n",
    "# Iterate through each row and append max similarity score as a new column\n",
    "current_events_df['Max_Similarity_Score'] = [get_max_similarity_score(row) for row in cosine_sim]\n",
    "\n",
    "# Drop the processed_description column as it's not needed for the final output\n",
    "current_events_df = current_events_df.drop(columns=['processed_description'])\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "# current_events_df\n",
    "\n",
    "threshold = 0.1\n",
    "recommended_events = current_events_df[current_events_df['Max_Similarity_Score'] > threshold]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "# recommended_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930b6c6-6515-4ae7-9268-6716cb728a1f",
   "metadata": {},
   "source": [
    "# Translate events table and add urls for buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0946f2c-0e79-4070-b383-d2fbd2306ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Initialize the Translator\n",
    "translator = Translator()\n",
    "\n",
    "# Translate the contents of the 'text' column to Spanish\n",
    "recommended_events['translated_text'] = recommended_events['Description'].apply(lambda x: translator.translate(x, src='de', dest='en').text)\n",
    "recommended_events['translated_date'] = recommended_events['Date'].apply(lambda x: translator.translate(x, src='de', dest='en').text)\n",
    "recommended_events['translated_title'] = recommended_events['Title'].apply(lambda x: translator.translate(x, src='de', dest='en').text)\n",
    "\n",
    "\n",
    "base_url = \"https://www.berlin.de\"\n",
    "\n",
    "# Append the base URL to each row in the 'link' and 'image' columns\n",
    "recommended_events['Link'] = base_url + recommended_events['Link']\n",
    "recommended_events['Image'] = base_url + recommended_events['Image']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37d046-0f61-4250-83f2-6d7bfa2bc0b6",
   "metadata": {},
   "source": [
    "# Creating target film data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac2d31-58ed-4a39-b165-e3865fd46f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://letterboxd.com/lukeinha27/films/rated/3.5-5/page/\"\n",
    "\n",
    "# Initialize an empty list to store the titles\n",
    "stans_titles = []\n",
    "\n",
    "# Loop through the first three pages\n",
    "for page_num in range(1, 4):\n",
    "    # Construct the full URL for the current page\n",
    "    url = f\"{base_url}{page_num}/\"\n",
    "    \n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract the titles from the 'alt' attribute of 'img' tags\n",
    "    titles = [img['alt'] for img in soup.find_all('img', alt=True)]\n",
    "    \n",
    "    # Add the titles to the list\n",
    "    stans_titles.extend(titles)\n",
    "\n",
    "# Display all the extracted titles\n",
    "stans_titles.remove('lukeinha27')\n",
    "\n",
    "chichi_url = \"https://letterboxd.com/chiarafury/films/rated/4-5/\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "chichi_response = requests.get(chichi_url)\n",
    "chichi_soup = BeautifulSoup(chichi_response.content, \"html.parser\")\n",
    "chichi_titles = [img['alt'] for img in chichi_soup.find_all('img', alt=True)]\n",
    "chichi_titles.remove('chiarafury')\n",
    "# chichi_titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed6b88-4ad4-4970-8b77-81a94426f9ce",
   "metadata": {},
   "source": [
    "# Collecting the film (target) data\n",
    "You can use this either to create the target or for the films that are currently on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fb4da-0e83-436b-8ebf-81c1cbc856f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_20_items = clean_target_movies_list[:20]\n",
    "second_20_items = clean_target_movies_list[21:40]\n",
    "third_20_items = clean_target_movies_list[41:60]\n",
    "# Do this for as much target data as you want. The full target list is 180 long but I didn't run it all because of API cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffab59-1314-4a1e-abcc-8122eb26d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for each of your shortened item list (first_20, second_20 etc) \n",
    "# Probably can do code that runs through each. Split it to avoid timeout issues - wouldn't work if you run the whole list at once\n",
    "# You can also get the imdb rating if you reveal the get ratings code in the middle\n",
    "\n",
    "def fetch_film_data(film_titles, output_table_name, api_key):\n",
    "    # RapidAPI details\n",
    "    api_host = \"imdb8.p.rapidapi.com\"\n",
    "\n",
    "    # Base URLs for the APIs\n",
    "    search_url = \"https://imdb8.p.rapidapi.com/v2/search\"\n",
    "    # ratings_url = \"https://imdb8.p.rapidapi.com/title/v2/get-ratings\"\n",
    "    keywords_url = \"https://imdb8.p.rapidapi.com/title/v2/get-keywords\"\n",
    "\n",
    "    # Headers for the API requests\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": api_key,\n",
    "        \"X-RapidAPI-Host\": api_host\n",
    "    }\n",
    "\n",
    "    # List to store the results\n",
    "    results = []\n",
    "\n",
    "    # Loop through the list of film titles\n",
    "    for title in film_titles:\n",
    "        # Query string for the search API\n",
    "        querystring = {\"searchTerm\": title, \"type\": \"MOVIE\", \"first\": \"1\", \"country\": \"US\", \"language\": \"en-US\"}\n",
    "\n",
    "        # Make the API call to search for the title\n",
    "        response = requests.get(search_url, headers=headers, params=querystring)\n",
    "        \n",
    "        # Adding delay between API calls\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if response.ok:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                edges = data[\"data\"][\"mainSearch\"][\"edges\"]\n",
    "                film_id = None\n",
    "                for edge in edges:\n",
    "                    node = edge.get(\"node\", {})\n",
    "                    entity = node.get(\"entity\", {})\n",
    "                    if entity.get(\"__typename\") == \"Title\":\n",
    "                        film_id = entity.get(\"id\", \"\")\n",
    "                        break\n",
    "            except (KeyError, json.JSONDecodeError) as e:\n",
    "                print(f\"Error processing search response for {title}: {e}\")\n",
    "                film_id = None\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.reason}\")\n",
    "            film_id = None\n",
    "        \n",
    "        if film_id:\n",
    "            # # Make the API call to get ratings\n",
    "            # querystring2 = {\"tconst\": film_id, \"country\": \"US\", \"language\": \"en-US\"}\n",
    "            # response2 = requests.get(ratings_url, headers=headers, params=querystring2)\n",
    "            \n",
    "            # # Adding delay between API calls\n",
    "            # time.sleep(1)\n",
    "            \n",
    "            # if response2.ok:\n",
    "            #     try:\n",
    "            #         data2 = response2.json()\n",
    "            #         aggregate_rating = data2[\"data\"][\"title\"][\"ratingsSummary\"][\"aggregateRating\"]\n",
    "            #     except (KeyError, json.JSONDecodeError) as e:\n",
    "            #         print(f\"Error processing ratings response for {film_id}: {e}\")\n",
    "            #         aggregate_rating = None\n",
    "            # else:\n",
    "            #     print(f\"Error: {response2.status_code} - {response2.reason}\")\n",
    "            #     aggregate_rating = None\n",
    "            \n",
    "            # # Make the API call to get keywords\n",
    "            # querystring3 = {\"tconst\": film_id, \"first\": \"20\", \"country\": \"US\", \"language\": \"en-US\"}\n",
    "            # response3 = requests.get(keywords_url, headers=headers, params=querystring3)\n",
    "            \n",
    "            # # Adding delay between API calls\n",
    "            # time.sleep(1)\n",
    "            \n",
    "            if response3.ok:\n",
    "                try:\n",
    "                    data3 = response3.json()\n",
    "                    keyword_item_categories = data3['data']['title']['keywordItemCategories']\n",
    "                    keywords = []\n",
    "                    for category in keyword_item_categories:\n",
    "                        category_keywords = category.get('keywords', {}).get('edges', [])\n",
    "                        for edge in category_keywords:\n",
    "                            keyword_text = edge.get('node', {}).get('keyword', {}).get('text', {}).get('text', '')\n",
    "                            if keyword_text:\n",
    "                                keywords.append(keyword_text)\n",
    "                except (KeyError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error processing keywords response for {film_id}: {e}\")\n",
    "                    keywords = []\n",
    "            else:\n",
    "                print(f\"Error: {response3.status_code} - {response3.reason}\")\n",
    "                keywords = []\n",
    "            \n",
    "            # Store the result in the results list\n",
    "            results.append({\n",
    "                \"Title\": title,\n",
    "                \"Film ID\": film_id,\n",
    "                # \"Aggregate Rating\": aggregate_rating,\n",
    "                \"Keywords\": keywords\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Film ID not found for {title}\")\n",
    "\n",
    "    # Create a DataFrame from the results and save it as a global variable\n",
    "    globals()[output_table_name] = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa9a97-a2cd-4530-99f4-92af6a818199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function \n",
    "output_table_name = \"on_films1\"\n",
    "fetch_film_data(first_20_items, output_table_name, api_key)\n",
    "\n",
    "# Print the DataFrame\n",
    "on_films1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673807a4-021a-4b29-8079-dd5587109b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rated_film_combo = pd.concat([rated_films1, rated_films2,rated_films3]).reset_index(drop=True)\n",
    "# Here is using 3 but you can do as many as you want for the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf75f2f-07d1-4647-93cd-8e68e2630160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enriching showing films with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cfef6f-8385-4c89-8df9-b5402363febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_films_joined = pd.concat([on_films1, on_films2,on_films3]).reset_index(drop=True)\n",
    "# Same for the on film, you can do as many as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be2408-1ea0-4b83-b0e4-1179bb287d37",
   "metadata": {},
   "source": [
    "# Matching the currently showing films to the target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ab35d-32ca-451f-9ba8-017c2dc7f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to preprocess text: tokenize, lemmatize, and remove stop words\n",
    "def preprocess(text):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the descriptions\n",
    "on_films_joined['processed_description'] = on_films_joined['Keywords'].apply(preprocess)\n",
    "target_rated_film_combo['processed_description'] = target_rated_film_combo['Keywords'].apply(preprocess)\n",
    "\n",
    "# Combine descriptions with target descriptions\n",
    "all_keywords = on_films_joined['processed_description'].tolist() + target_rated_film_combo['processed_description'].tolist()\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform all descriptions\n",
    "tfidf_matrix = vectorizer.fit_transform(all_keywords)\n",
    "\n",
    "# Separate the tfidf matrix into event descriptions and target descriptions\n",
    "movie_tfidf_matrix = tfidf_matrix[:len(on_films_joined)]\n",
    "movie_target_tfidf_matrix = tfidf_matrix[len(on_films_joined):]\n",
    "\n",
    "# Compute cosine similarity between event descriptions and target descriptions\n",
    "movie_cosine_sim = cosine_similarity(movie_tfidf_matrix, movie_target_tfidf_matrix)\n",
    "\n",
    "# Function to get max similarity score for each event description\n",
    "def get_max_similarity_score(cosine_sim_row):\n",
    "    return max(cosine_sim_row)\n",
    "\n",
    "# Iterate through each row and append max similarity score as a new column\n",
    "on_films_joined['Max_Similarity_Score'] = [get_max_similarity_score(row) for row in movie_cosine_sim]\n",
    "\n",
    "# Drop the processed_description column as it's not needed for the final output\n",
    "on_films_joined = on_films_joined.drop(columns=['processed_description'])\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "# on_films_joined\n",
    "\n",
    "threshold2 = 0.3\n",
    "recommended_films = on_films_joined[on_films_joined['Max_Similarity_Score'] > threshold2]\n",
    "\n",
    "recommended_films = recommended_films[recommended_films['Aggregate Rating'] > 6.4]\n",
    "# recommended_films"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64f3d8-e5f7-41a2-90a3-b80307139d47",
   "metadata": {},
   "source": [
    "You need to join your tables, \n",
    "- recommended_films is your main table and you add the image url from top_rated_films \n",
    "- alternatively change your code so that you only use one api to retrieve the id and then use both other apis the second time around and retrieve keywords, rating and image url together\n",
    "\n",
    "Then you need to generate a link for the button "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2423c-85ec-4490-87f8-3473b4fa1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "base_url = \"https://www.yorck.de/en/films/\"\n",
    "\n",
    "# Function to generate link from title\n",
    "def generate_link(title):\n",
    "    # Clean title, convert to lowercase, and replace spaces with hyphens\n",
    "    cleaned_title = title.strip().lower().replace(\" \", \"-\")\n",
    "    # Encode the title to handle special characters\n",
    "    encoded_title = urllib.parse.quote(cleaned_title)\n",
    "    # Concatenate the base URL and encoded title\n",
    "    link = base_url + encoded_title + \"?\"\n",
    "    return link\n",
    "\n",
    "\n",
    "# Apply the function to generate links\n",
    "recommended_films['Link'] = recommended_films['Title'].apply(generate_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed213c16-e923-4933-afdb-4de6e330dc86",
   "metadata": {},
   "source": [
    "# Get Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f543405-3a49-40b3-b3bc-ffffaaee2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_film_data(film_ids, api_key):\n",
    "    api_host = \"imdb8.p.rapidapi.com\"\n",
    "    plots_url = \"https://imdb8.p.rapidapi.com/title/v2/get-plots\"\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": api_key,\n",
    "        \"X-RapidAPI-Host\": api_host\n",
    "    }\n",
    "\n",
    "    titles_list = []\n",
    "    plots_list = []\n",
    "    summaries_list = []\n",
    "\n",
    "    for film_id in film_ids:\n",
    "        success = False\n",
    "        retries = 3\n",
    "        delay = 1  # Start with 1 second delay\n",
    "\n",
    "        while not success and retries > 0:\n",
    "            try:\n",
    "                querystring = {\"tconst\": film_id}\n",
    "                response = requests.get(plots_url, headers=headers, params=querystring)\n",
    "                \n",
    "                if response.status_code == 403:\n",
    "                    print(f\"Access forbidden for film ID {film_id}: Check your API key and permissions.\")\n",
    "                    titles_list.append('N/A')\n",
    "                    plots_list.append('N/A')\n",
    "                    summaries_list.append([])\n",
    "                    success = True\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Rate limit hit for film ID {film_id}: Retrying after delay.\")\n",
    "                    retries -= 1\n",
    "                    time.sleep(delay)\n",
    "                    delay *= 2  # Exponential backoff\n",
    "                elif response.ok:\n",
    "                    data = response.json()\n",
    "\n",
    "                    # Debugging the structure of the JSON response\n",
    "                    print(f\"Response for {film_id}: {data}\")\n",
    "\n",
    "                    title = data.get('data', {}).get('title', {}).get('titleText', {}).get('text', 'N/A')\n",
    "                    plot = data.get('data', {}).get('title', {}).get('plot', {}).get('plotText', {}).get('plainText', 'N/A')\n",
    "                    plots_edges = data.get('data', {}).get('title', {}).get('plots', {}).get('edges', [])\n",
    "                    summaries = [edge.get('node', {}).get('plotText', {}).get('plainText', 'N/A') for edge in plots_edges]\n",
    "\n",
    "                    titles_list.append(title)\n",
    "                    plots_list.append(plot)\n",
    "                    summaries_list.append(summaries)\n",
    "                    success = True\n",
    "                else:\n",
    "                    print(f\"Error fetching data for film ID {film_id}: {response.status_code}\")\n",
    "                    titles_list.append('N/A')\n",
    "                    plots_list.append('N/A')\n",
    "                    summaries_list.append([])\n",
    "                    success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for film ID {film_id}: {e}\")\n",
    "                titles_list.append('N/A')\n",
    "                plots_list.append('N/A')\n",
    "                summaries_list.append([])\n",
    "                success = True\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Film ID': film_ids,\n",
    "        'Title': titles_list,\n",
    "        'Plot': plots_list,\n",
    "        'Summaries': summaries_list\n",
    "    })\n",
    "\n",
    "# Example usage\n",
    "\n",
    "film_ids = recommended_films['Film ID'].tolist()  # Assuming 'recommended_films' is your input DataFrame\n",
    "\n",
    "# Fetch data and create new DataFrame\n",
    "plot_data = fetch_film_data(film_ids, api_key)\n",
    "plot_data = plot_data.drop(columns=['Summaries'])\n",
    "plot_data = plot_data.rename(columns={'Title': 'OGTitle'})\n",
    "recommended_films_final = recommended_films\n",
    "recommended_films_mega_final = recommended_films_final.merge(plot_data, on='Film ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e705e-cb79-4b77-95e1-1408f7763a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_films_final.to_json('film_recs.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
